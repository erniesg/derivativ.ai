"""
Document Storage Repository.
Handles database operations for document metadata and file references.
"""

import contextlib
import logging
from datetime import datetime
from typing import Optional
from uuid import UUID

from src.core.config import get_settings
from src.models.stored_document_models import (
    DocumentFile,
    DocumentSearchFilters,
    StoredDocument,
    StoredDocumentMetadata,
)
from supabase import Client

logger = logging.getLogger(__name__)


class DocumentStorageError(Exception):
    """Raised when document storage operations fail."""

    pass


class DocumentStorageRepository:
    """Repository for document storage database operations."""

    def __init__(self, supabase_client: Client):
        """
        Initialize document storage repository.

        Args:
            supabase_client: Supabase client instance
        """
        self.client = supabase_client
        settings = get_settings()
        self.table_prefix = settings.table_prefix

    async def save_document_metadata(self, metadata: StoredDocumentMetadata) -> UUID:
        """
        Save document metadata to database.

        Args:
            metadata: Document metadata to save

        Returns:
            Document ID of saved metadata

        Raises:
            DocumentStorageError: If save operation fails
        """
        try:
            # Prepare data for insertion - exclude computed fields and convert types
            data = metadata.model_dump(
                exclude={"search_content"}
            )  # search_content is auto-generated by DB
            data["id"] = str(data["id"])
            if data.get("session_id"):
                data["session_id"] = str(data["session_id"])

            # Convert datetime objects to ISO format strings
            if data.get("created_at"):
                data["created_at"] = data["created_at"].isoformat()
            if data.get("updated_at"):
                data["updated_at"] = data["updated_at"].isoformat()
            if data.get("deleted_at"):
                data["deleted_at"] = data["deleted_at"].isoformat()

            # Insert into stored_documents table
            response = (
                self.client.table(f"{self.table_prefix}stored_documents").insert(data).execute()
            )

            if not response.data:
                raise DocumentStorageError("Failed to save document metadata - no data returned")

            logger.info(f"Successfully saved document metadata: {metadata.id}")
            return metadata.id

        except Exception as e:
            logger.error(f"Failed to save document metadata: {e}")
            raise DocumentStorageError(f"Failed to save document metadata: {e}")

    async def retrieve_document_by_id(self, document_id: UUID) -> Optional[StoredDocument]:
        """
        Retrieve document by ID with associated files.

        Args:
            document_id: Document ID to retrieve

        Returns:
            StoredDocument if found, None otherwise

        Raises:
            DocumentStorageError: If retrieval operation fails
        """
        try:
            # Get document metadata
            metadata_response = (
                self.client.table(f"{self.table_prefix}stored_documents")
                .select("*")
                .eq("id", str(document_id))
                .execute()
            )

            if not metadata_response.data:
                return None

            metadata_data = metadata_response.data[0]

            # Convert string UUIDs back to UUID objects if they are strings
            if metadata_data.get("id") and isinstance(metadata_data["id"], str):
                metadata_data["id"] = UUID(metadata_data["id"])
            if metadata_data.get("session_id") and isinstance(metadata_data["session_id"], str):
                metadata_data["session_id"] = UUID(metadata_data["session_id"])

            metadata = StoredDocumentMetadata(**metadata_data)

            # Get associated files
            files = await self.get_document_files(document_id)

            # Get session data if available
            session_data = {}
            if metadata.session_id:
                session_response = (
                    self.client.table("generation_sessions")
                    .select("*")
                    .eq("session_id", str(metadata.session_id))
                    .execute()
                )
                if session_response.data:
                    session_data = session_response.data[0]

            return StoredDocument(metadata=metadata, files=files, session_data=session_data)

        except Exception as e:
            logger.error(f"Failed to retrieve document {document_id}: {e}")
            raise DocumentStorageError(f"Failed to retrieve document: {e}")

    async def update_document_status(
        self, document_id: UUID, status: str, metadata: Optional[dict] = None
    ) -> bool:
        """
        Update document status and optional metadata.

        Args:
            document_id: Document ID to update
            status: New status value
            metadata: Optional metadata to merge

        Returns:
            True if update successful

        Raises:
            DocumentStorageError: If update operation fails
        """
        try:
            update_data = {"status": status, "updated_at": datetime.now().isoformat()}

            if metadata:
                # Merge with existing metadata if needed
                update_data.update(metadata)

            response = (
                self.client.table(f"{self.table_prefix}stored_documents")
                .update(update_data)
                .eq("id", str(document_id))
                .execute()
            )

            if not response.data:
                raise DocumentStorageError("Document not found or update failed")

            logger.info(f"Successfully updated document {document_id} status to {status}")
            return True

        except Exception as e:
            logger.error(f"Failed to update document status: {e}")
            raise DocumentStorageError(f"Failed to update document status: {e}")

    async def soft_delete_document(self, document_id: UUID) -> bool:
        """
        Soft delete document by marking as deleted.

        Args:
            document_id: Document ID to delete

        Returns:
            True if deletion successful

        Raises:
            DocumentStorageError: If deletion operation fails
        """
        try:
            update_data = {
                "status": "deleted",
                "deleted_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
            }

            response = (
                self.client.table(f"{self.table_prefix}stored_documents")
                .update(update_data)
                .eq("id", str(document_id))
                .execute()
            )

            if not response.data:
                raise DocumentStorageError("Document not found or deletion failed")

            logger.info(f"Successfully soft deleted document: {document_id}")
            return True

        except Exception as e:
            logger.error(f"Failed to soft delete document: {e}")
            raise DocumentStorageError(f"Failed to soft delete document: {e}")

    async def search_documents(self, filters: DocumentSearchFilters) -> dict[str, any]:
        """
        Search documents with filters and pagination.

        Args:
            filters: Search filters and pagination options

        Returns:
            Dictionary with search results and metadata

        Raises:
            DocumentStorageError: If search operation fails
        """
        try:
            # Build query
            query = self.client.table(f"{self.table_prefix}stored_documents").select(
                "*", count="exact"
            )

            # Apply filters
            query = query.neq("status", "deleted")  # Exclude deleted documents

            if filters.document_type:
                query = query.eq("document_type", filters.document_type)

            if filters.topic:
                query = query.eq("topic", filters.topic)

            if filters.grade_level:
                query = query.eq("grade_level", filters.grade_level)

            if filters.status:
                query = query.eq("status", filters.status)

            if filters.search_text:
                query = query.ilike("search_content", f"%{filters.search_text}%")

            if filters.tags:
                # Search for documents that contain any of the specified tags
                for tag in filters.tags:
                    query = query.contains("tags", [tag])

            if filters.created_after:
                query = query.gte("created_at", filters.created_after.isoformat())

            if filters.created_before:
                query = query.lte("created_at", filters.created_before.isoformat())

            # Apply pagination
            query = query.limit(filters.limit).offset(filters.offset)

            # Execute query
            response = query.execute()

            # Convert results to StoredDocumentMetadata objects
            documents = []
            for doc_data in response.data:
                # Convert string UUIDs back to UUID objects if they are strings
                if doc_data.get("id") and isinstance(doc_data["id"], str):
                    doc_data["id"] = UUID(doc_data["id"])
                if doc_data.get("session_id") and isinstance(doc_data["session_id"], str):
                    doc_data["session_id"] = UUID(doc_data["session_id"])

                documents.append(StoredDocumentMetadata(**doc_data))

            # Calculate total count and has_more
            total_count = response.count if response.count is not None else len(documents)
            has_more = (filters.offset + filters.limit) < total_count

            return {
                "documents": documents,
                "total_count": total_count,
                "offset": filters.offset,
                "limit": filters.limit,
                "has_more": has_more,
            }

        except Exception as e:
            logger.error(f"Failed to search documents: {e}")
            raise DocumentStorageError(f"Failed to search documents: {e}")

    async def save_document_file(self, file: DocumentFile) -> UUID:
        """
        Save document file information to database.

        Args:
            file: Document file information to save

        Returns:
            File ID of saved file

        Raises:
            DocumentStorageError: If save operation fails
        """
        try:
            # Prepare data for insertion
            data = file.model_dump()
            data["id"] = str(data["id"])
            data["document_id"] = str(data["document_id"])

            # Convert datetime objects to ISO format strings
            if data.get("created_at"):
                data["created_at"] = data["created_at"].isoformat()
            if data.get("updated_at"):
                data["updated_at"] = data["updated_at"].isoformat()

            # Insert into document_files table
            response = (
                self.client.table(f"{self.table_prefix}document_files").insert(data).execute()
            )

            if not response.data:
                raise DocumentStorageError("Failed to save document file - no data returned")

            logger.info(f"Successfully saved document file: {file.id}")
            return file.id

        except Exception as e:
            logger.error(f"Failed to save document file: {e}")
            raise DocumentStorageError(f"Failed to save document file: {e}")

    async def get_document_files(self, document_id: UUID) -> list[DocumentFile]:
        """
        Get all files associated with a document.

        Args:
            document_id: Document ID to get files for

        Returns:
            List of DocumentFile objects

        Raises:
            DocumentStorageError: If retrieval operation fails
        """
        try:
            response = (
                self.client.table(f"{self.table_prefix}document_files")
                .select("*")
                .eq("document_id", str(document_id))
                .execute()
            )

            files = []
            for file_data in response.data:
                # Convert string UUIDs back to UUID objects if they are strings
                if file_data.get("id") and isinstance(file_data["id"], str):
                    file_data["id"] = UUID(file_data["id"])
                if file_data.get("document_id") and isinstance(file_data["document_id"], str):
                    file_data["document_id"] = UUID(file_data["document_id"])

                files.append(DocumentFile(**file_data))

            return files

        except Exception as e:
            logger.error(f"Failed to get document files: {e}")
            raise DocumentStorageError(f"Failed to get document files: {e}")

    async def update_file_storage_info(self, file_id: UUID, storage_info: dict[str, any]) -> bool:
        """
        Update file storage information.

        Args:
            file_id: File ID to update
            storage_info: Storage information to update

        Returns:
            True if update successful

        Raises:
            DocumentStorageError: If update operation fails
        """
        try:
            update_data = {"r2_metadata": storage_info, "updated_at": datetime.now().isoformat()}

            # Update file size if provided
            if "file_size" in storage_info:
                update_data["file_size"] = storage_info["file_size"]

            response = (
                self.client.table(f"{self.table_prefix}document_files")
                .update(update_data)
                .eq("id", str(file_id))
                .execute()
            )

            if not response.data:
                raise DocumentStorageError("File not found or update failed")

            logger.info(f"Successfully updated file storage info: {file_id}")
            return True

        except Exception as e:
            logger.error(f"Failed to update file storage info: {e}")
            raise DocumentStorageError(f"Failed to update file storage info: {e}")

    async def get_documents_by_session_id(self, session_id: UUID) -> list[StoredDocumentMetadata]:
        """
        Get all documents associated with a generation session.

        Args:
            session_id: Session ID to get documents for

        Returns:
            List of StoredDocumentMetadata objects

        Raises:
            DocumentStorageError: If retrieval operation fails
        """
        try:
            response = (
                self.client.table(f"{self.table_prefix}stored_documents")
                .select("*")
                .eq("session_id", str(session_id))
                .execute()
            )

            documents = []
            for doc_data in response.data:
                # Convert string UUIDs back to UUID objects if they are strings
                if doc_data.get("id") and isinstance(doc_data["id"], str):
                    doc_data["id"] = UUID(doc_data["id"])
                if doc_data.get("session_id") and isinstance(doc_data["session_id"], str):
                    doc_data["session_id"] = UUID(doc_data["session_id"])

                documents.append(StoredDocumentMetadata(**doc_data))

            return documents

        except Exception as e:
            logger.error(f"Failed to get documents by session ID: {e}")
            raise DocumentStorageError(f"Failed to get documents by session ID: {e}")

    async def get_document_statistics(self) -> dict[str, any]:
        """
        Get statistics about stored documents.

        Returns:
            Dictionary with document statistics

        Raises:
            DocumentStorageError: If statistics calculation fails
        """
        try:
            # Use Supabase RPC function for complex statistics
            # For now, implement basic statistics

            # Get total document count
            total_response = (
                self.client.table(f"{self.table_prefix}stored_documents")
                .select("*", count="exact")
                .neq("status", "deleted")
                .execute()
            )
            total_documents = total_response.count or 0

            # Get total file size
            size_response = (
                self.client.table(f"{self.table_prefix}stored_documents")
                .select("total_file_size")
                .neq("status", "deleted")
                .execute()
            )
            total_file_size = sum(doc.get("total_file_size", 0) for doc in size_response.data)

            # Get documents by type
            type_response = (
                self.client.table(f"{self.table_prefix}stored_documents")
                .select("document_type")
                .neq("status", "deleted")
                .execute()
            )
            documents_by_type = {}
            for doc in type_response.data:
                doc_type = doc.get("document_type", "unknown")
                documents_by_type[doc_type] = documents_by_type.get(doc_type, 0) + 1

            # Get documents by status
            status_response = (
                self.client.table(f"{self.table_prefix}stored_documents")
                .select("status")
                .neq("status", "deleted")
                .execute()
            )
            documents_by_status = {}
            for doc in status_response.data:
                status = doc.get("status", "unknown")
                documents_by_status[status] = documents_by_status.get(status, 0) + 1

            return {
                "total_documents": total_documents,
                "total_file_size": total_file_size,
                "documents_by_type": documents_by_type,
                "documents_by_status": documents_by_status,
                "average_file_size": total_file_size // max(total_documents, 1),
                "generation_success_rate": documents_by_status.get("exported", 0)
                / max(total_documents, 1),
            }

        except Exception as e:
            logger.error(f"Failed to get document statistics: {e}")
            raise DocumentStorageError(f"Failed to get document statistics: {e}")

    async def cleanup_old_documents(self, cutoff_date: datetime) -> int:
        """
        Mark old documents as deleted.

        Args:
            cutoff_date: Documents created before this date will be deleted

        Returns:
            Number of documents deleted

        Raises:
            DocumentStorageError: If cleanup operation fails
        """
        try:
            update_data = {
                "status": "deleted",
                "deleted_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
            }

            response = (
                self.client.table(f"{self.table_prefix}stored_documents")
                .update(update_data)
                .lt("created_at", cutoff_date.isoformat())
                .execute()
            )

            deleted_count = len(response.data) if response.data else 0

            logger.info(f"Successfully cleaned up {deleted_count} old documents")
            return deleted_count

        except Exception as e:
            logger.error(f"Failed to cleanup old documents: {e}")
            raise DocumentStorageError(f"Failed to cleanup old documents: {e}")

    def validate_document_metadata(self, metadata: StoredDocumentMetadata) -> bool:
        """
        Validate document metadata before saving.

        Args:
            metadata: Document metadata to validate

        Returns:
            True if valid, False otherwise
        """
        try:
            # Check required fields
            if not metadata.title or not metadata.title.strip():
                return False

            if not metadata.document_type:
                return False

            # Validate enums
            valid_types = ["worksheet", "notes", "textbook", "slides"]
            if metadata.document_type not in valid_types:
                return False

            valid_statuses = [
                "pending",
                "generating",
                "generated",
                "exporting",
                "exported",
                "failed",
                "deleted",
                "archived",
            ]
            return metadata.status in valid_statuses

        except Exception:
            return False

    def generate_search_content(self, metadata: StoredDocumentMetadata) -> str:
        """
        Generate searchable content from document metadata.

        Args:
            metadata: Document metadata

        Returns:
            Searchable content string
        """
        content_parts = [
            metadata.title,
            metadata.document_type,
            metadata.topic or "",
            " ".join(metadata.tags),
        ]
        return " ".join(filter(None, content_parts)).lower()

    def parse_document_filters(self, raw_filters: dict[str, any]) -> DocumentSearchFilters:
        """
        Parse raw filter parameters into DocumentSearchFilters.

        Args:
            raw_filters: Raw filter parameters

        Returns:
            Parsed DocumentSearchFilters object
        """
        try:
            # Parse grade level
            grade_level = None
            if raw_filters.get("grade_level"):
                with contextlib.suppress(ValueError, TypeError):
                    grade_level = int(raw_filters["grade_level"])

            # Parse dates
            created_after = None
            created_before = None

            if raw_filters.get("created_after"):
                with contextlib.suppress(ValueError, TypeError):
                    created_after = datetime.fromisoformat(
                        raw_filters["created_after"].replace("Z", "+00:00")
                    )

            if raw_filters.get("created_before"):
                with contextlib.suppress(ValueError, TypeError):
                    created_before = datetime.fromisoformat(
                        raw_filters["created_before"].replace("Z", "+00:00")
                    )

            # Parse tags
            tags = []
            if raw_filters.get("tags"):
                if isinstance(raw_filters["tags"], str):
                    tags = [tag.strip() for tag in raw_filters["tags"].split(",") if tag.strip()]
                elif isinstance(raw_filters["tags"], list):
                    tags = raw_filters["tags"]

            # Parse pagination
            limit = 50
            offset = 0

            with contextlib.suppress(ValueError, TypeError):
                limit = min(int(raw_filters.get("limit", 50)), 100)

            with contextlib.suppress(ValueError, TypeError):
                offset = max(int(raw_filters.get("offset", 0)), 0)

            return DocumentSearchFilters(
                document_type=raw_filters.get("document_type"),
                topic=raw_filters.get("topic"),
                grade_level=grade_level,
                status=raw_filters.get("status"),
                search_text=raw_filters.get("search_text", ""),
                tags=tags,
                created_after=created_after,
                created_before=created_before,
                limit=limit,
                offset=offset,
            )

        except Exception as e:
            logger.warning(f"Failed to parse document filters: {e}")
            return DocumentSearchFilters()  # Return default filters
